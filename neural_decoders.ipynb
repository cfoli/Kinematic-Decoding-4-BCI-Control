{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zfYvuo07tawM",
        "A0S6lOw_6Y9H",
        "gxVpCVKG2uaa",
        "nloWjoIzGQ9_",
        "ahchvJ5gMTM-",
        "l7UF-x4lPXER",
        "EnOGsDmxGlhe"
      ],
      "mount_file_id": "15vJRZhcrtSLQrMmU_J_eJZX5HmW5n1x_",
      "authorship_tag": "ABX9TyO3o0w6akNnk2oZbYozm8qZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cfoli/Kinematic-Decoding-4-BCI-Control/blob/main/neural_decoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# started: Mar 30.2025.Su (by Crispin Foli)"
      ],
      "metadata": {
        "id": "c-u3ALlDqLMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "ZKkJXGIEuTWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def append_history(neural_data, targ_data=None, num_bins_before=1, mode='flatten'):\n",
        "    \"\"\"\n",
        "    Constructs a history matrix from neural data for causal decoding,\n",
        "    supporting both feedforward and LSTM-style output shapes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neural_data : np.ndarray of shape (T, N)\n",
        "        Neural features over time.\n",
        "\n",
        "    targ_data : np.ndarray of shape (T, m), optional\n",
        "        Corresponding target data, aligned with valid rows of neural data.\n",
        "\n",
        "    num_bins_before : int, optional (default=1)\n",
        "        Number of past time bins to include before the current bin.\n",
        "\n",
        "    mode : str, either 'flatten' or 'expand'\n",
        "        'flatten' returns (T-k, (k+1)*N) for feedforward networks.\n",
        "        'expand' returns (T-k, k+1, N) for LSTM or RNN input.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : np.ndarray\n",
        "        Time-lagged neural features of shape:\n",
        "            - (T - num_bins_before, (num_bins_before + 1) * N) if mode='flatten'\n",
        "            - (T - num_bins_before, num_bins_before + 1, N) if mode='expand'\n",
        "\n",
        "    y : np.ndarray of shape (T - num_bins_before, m) or None\n",
        "        Aligned target data, if provided.\n",
        "    \"\"\"\n",
        "    if num_bins_before < 0:\n",
        "        raise ValueError(\"num_bins_before must be non-negative\")\n",
        "    if mode not in ['flatten', 'expand']:\n",
        "        raise ValueError(\"mode must be 'flatten' or 'expand'\")\n",
        "    if neural_data.ndim != 2:\n",
        "        raise ValueError(\"neural_data must be 2D (time, features)\")\n",
        "\n",
        "    T, N = neural_data.shape\n",
        "    window_size = num_bins_before + 1\n",
        "\n",
        "    if T <= num_bins_before:\n",
        "        raise ValueError(\"Not enough time bins to apply specified history length.\")\n",
        "\n",
        "    # Create full lagged matrix\n",
        "    X_full = np.full((T, window_size, N), np.nan)\n",
        "\n",
        "    for t in range(num_bins_before, T):\n",
        "        X_full[t] = neural_data[t - num_bins_before : t + 1]\n",
        "\n",
        "    # Drop initial rows with NaNs\n",
        "    X_valid = X_full[num_bins_before:]\n",
        "\n",
        "    # Return in desired mode\n",
        "    if mode == 'flatten':\n",
        "        X_out = X_valid.reshape(X_valid.shape[0], -1)\n",
        "    else:  # mode == 'expand'\n",
        "        X_out = X_valid\n",
        "\n",
        "    y_out = targ_data[num_bins_before:, :] if targ_data is not None else None\n",
        "\n",
        "    return X_out, y_out\n"
      ],
      "metadata": {
        "id": "zNlCl1hQuXk6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def append_history_cnn(neural_data, targ_data=None, num_bins_before=1):\n",
        "    \"\"\"\n",
        "    Appends time-lagged history to the neural input.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neural_data : ndarray, shape (T, N)\n",
        "        Raw neural activity over time (time x channels).\n",
        "    targ_data : ndarray, shape (T, D), optional\n",
        "        Target values (e.g., kinematics) over time.\n",
        "    num_bins_before : int\n",
        "        Number of previous bins to include in addition to the current bin.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : ndarray, shape (T - num_bins_before, num_bins_before + 1, N)\n",
        "        History-augmented neural input.\n",
        "    y : ndarray or None\n",
        "        Trimmed target array, or None if targ_data not provided.\n",
        "    \"\"\"\n",
        "    T, N = neural_data.shape\n",
        "    window = num_bins_before + 1\n",
        "    X = np.full((T, window, N), np.nan)\n",
        "\n",
        "    for i in range(num_bins_before, T):\n",
        "        X[i] = neural_data[i - num_bins_before:i + 1]\n",
        "\n",
        "    X = X[num_bins_before:]\n",
        "    y = targ_data[num_bins_before:] if targ_data is not None else None\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "TSESjA8nuboM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Kalman Filter\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zfYvuo07tawM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class KalmanFilter(object):\n",
        "    \"\"\"\n",
        "    Kalman Filter for neural decoding based on the Kording/Wu et al. 2003 framework.\n",
        "\n",
        "    Adds support for using a fixed initial state (`z0`) and initial uncertainty (`P0`)\n",
        "    computed from training data. This avoids dependence on ground-truth during inference.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    C : float\n",
        "        Scaling factor for process noise (W). Controls how much to trust the dynamics.\n",
        "\n",
        "    model : dict\n",
        "        Contains trained parameters:\n",
        "            - A: State transition matrix\n",
        "            - W: Process noise covariance\n",
        "            - H: Observation matrix\n",
        "            - Q: Observation noise covariance\n",
        "            - z0: Initial state mean (column vector)\n",
        "            - P0: Initial state covariance matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, C=1, O=1):\n",
        "        \"\"\"\n",
        "        Initializes the Kalman filter object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        C : float, optional (default=1)\n",
        "            Scaling factor for process noise (W). Increasing C reduces the weight of\n",
        "            the dynamics (A) during filtering.\n",
        "        O : int, optional (default=1)\n",
        "            Autoregressive model order for the state model (must be >= 1). O controls how many past time steps are used to predict the next state. A larger O models long-term temporal dependencies.\n",
        "        \"\"\"\n",
        "        self.C = C\n",
        "        assert O >= 1, \"Autoregressive order O must be >= 1\"\n",
        "        self.O = O\n",
        "\n",
        "        self.data_preprocessor = StandardScaler() # Z-score neural data\n",
        "        self.targ_transformer  = StandardScaler(with_std=False) # Zero-center target data\n",
        "\n",
        "        self.used_PCA_TF = False # flag to tell predict() whether or not training used PCA\n",
        "\n",
        "    def fit(self, X_train, y_train, use_PCA_TF=False, PC_thresh=None):\n",
        "        \"\"\"\n",
        "        Train the Kalman Filter model using paired neural and behavioral data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray of shape [n_samples, n_neurons]\n",
        "            Neural features (observations).\n",
        "        use_PCA_TF : boolean scalar\n",
        "            Transform neural data using PCA if true?\n",
        "        PC_thresh : float or int scalar\n",
        "            If int: Number of principal components to keep.\n",
        "            If float: Proportion of variance to keep.\n",
        "        y_train : ndarray of shape [n_samples, n_outputs]\n",
        "            Behavioral variables (true hidden states).\n",
        "        \"\"\"\n",
        "\n",
        "        # Run PCA if true\n",
        "        if use_PCA_TF:\n",
        "          self.used_PCA_TF = True\n",
        "          self.num_PCs = None\n",
        "          from sklearn.decomposition import PCA\n",
        "          self.pca_transformer = PCA()\n",
        "          self.pca_transformer.fit(X_train)\n",
        "\n",
        "          if isinstance(PC_thresh,float) and PC_thresh>0:\n",
        "              explained_var_cumsum = np.cumsum(self.pca_transformer.explained_variance_ratio_)\n",
        "              self.num_PCs = np.where(explained_var_cumsum <= PC_thresh)[0][-1]\n",
        "          elif isinstance(PC_thresh,int) and PC_thresh>0:\n",
        "              self.num_PCs = PC_thresh\n",
        "\n",
        "          X_train = self.pca_transformer.transform(X_train)[:, :self.num_PCs]\n",
        "\n",
        "        # scale neural features\n",
        "        X_train   = self.data_preprocessor.fit_transform(X_train)\n",
        "        y_train   = self.targ_transformer.fit_transform(y_train)\n",
        "\n",
        "        X  = np.matrix(y_train.T)  # Hidden state (e.g., kinematics)\n",
        "        Z  = np.matrix(X_train.T)  # Observations (e.g., spikes)\n",
        "        nt = X.shape[1]  # Number of time steps\n",
        "\n",
        "        # Fit state transition matrix (A) and process noise (W)\n",
        "        o = self.O  # autoregressive model order\n",
        "        X2 = X[:, o:]\n",
        "        X1 = X[:, 0:nt - o]\n",
        "        A = X2 @ X1.T @ inv(X1 @ X1.T)\n",
        "        W = (X2 - A @ X1) @ (X2 - A @ X1).T / (nt - o) / self.C\n",
        "\n",
        "        # Fit observation matrix (H) and observation noise (Q)\n",
        "        H = Z @ X.T @ inv(X @ X.T)\n",
        "        Q = (Z - H @ X) @ (Z - H @ X).T / nt\n",
        "\n",
        "        # Initial state: use first state from training\n",
        "        z0 = np.asarray(X[:, 0]).flatten()  # Initial state vector\n",
        "\n",
        "        # Initial uncertainty: small identity matrix\n",
        "        P0 = np.eye(X.shape[0]) * 1e-3\n",
        "\n",
        "        # Store model parameters\n",
        "        self.model = {\n",
        "            'A': A,\n",
        "            'W': W,\n",
        "            'H': H,\n",
        "            'Q': Q,\n",
        "            'z0': z0,\n",
        "            'P0': P0\n",
        "        }\n",
        "\n",
        "    def predict(self, X_pred):\n",
        "        \"\"\"\n",
        "        Predict hidden states from neural observations using the trained Kalman Filter.\n",
        "\n",
        "        This method initializes the filter with the precomputed `z0` and `P0` from training.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_pred : ndarray of shape [n_samples, n_neurons]\n",
        "            Neural features to decode.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray of shape [n_samples, n_outputs]\n",
        "            Estimated hidden states (e.g., positions or velocities).\n",
        "        \"\"\"\n",
        "\n",
        "        if self.used_PCA_TF:\n",
        "          X_pred = self.pca_transformer.transform(X_pred)[:, :self.num_PCs]\n",
        "\n",
        "        X_pred = self.data_preprocessor.transform(X_pred)\n",
        "\n",
        "        # Load model parameters\n",
        "        A = self.model['A']\n",
        "        W = self.model['W']\n",
        "        H = self.model['H']\n",
        "        Q = self.model['Q']\n",
        "        z0 = self.model['z0']\n",
        "        P0 = self.model['P0']\n",
        "\n",
        "        Z = np.matrix(X_pred.T)\n",
        "        num_states = A.shape[0]\n",
        "        num_timesteps = Z.shape[1]\n",
        "\n",
        "        # Initialize state and covariance\n",
        "        states = np.empty((num_states, num_timesteps))\n",
        "        P = np.matrix(P0)\n",
        "        state = np.matrix(z0).T  # Ensure column vector\n",
        "\n",
        "        # Set initial state\n",
        "        states[:, 0] = np.asarray(state).flatten()\n",
        "\n",
        "        # Kalman filtering loop\n",
        "        for t in range(num_timesteps - 1):\n",
        "            # Predict next state\n",
        "            P_m = A @ P @ A.T + W\n",
        "            state_m = A @ state\n",
        "\n",
        "            # Kalman gain\n",
        "            K = P_m @ H.T @ np.linalg.inv(H @ P_m @ H.T + Q)\n",
        "\n",
        "            # Update state\n",
        "            state = state_m + K @ (Z[:, t + 1] - H @ state_m)\n",
        "            P = (np.eye(num_states) - K @ H) @ P_m\n",
        "\n",
        "            # Store prediction\n",
        "            states[:, t + 1] = np.asarray(state).flatten()\n",
        "\n",
        "        # ----------- Aliases -----------\n",
        "        # H is sometimes denoted as C in literature, and represents the transformation that converts kinematics to neural features\n",
        "        # Z is sometimes denoted y and represents the observed neural features\n",
        "        # -------------------------------\n",
        "\n",
        "        # Return predicted states (time x features)...after undoing zero-centering\n",
        "        return self.targ_transformer.inverse_transform(states.T)\n"
      ],
      "metadata": {
        "id": "VgnjQciKN3yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReFIT-Kalman Filter\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A0S6lOw_6Y9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "\n",
        "class ReFITKalmanFilter(KalmanFilter):\n",
        "    \"\"\"\n",
        "    ReFIT Kalman Filter (Gilja et al., 2012) using intention-aligned training and optional\n",
        "    position feedback correction during decoding.\n",
        "\n",
        "    This subclass extends KalmanFilter_zeroInit by:\n",
        "\n",
        "    1. Modifying the training targets to reflect intended velocities based on position and target.\n",
        "    2. Supporting velocity reorientation methods: \"flip\", \"rotate\", or \"both\".\n",
        "    3. Allowing optional velocity zeroing when inside the target.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    C : float\n",
        "        Scaling factor for process noise in KalmanFilter_zeroInit.\n",
        "    alpha : float\n",
        "        Velocity blending factor during training (used for rotation).\n",
        "    velocity_rot_method : str\n",
        "        Velocity correction method: 'flip', 'rotate', or 'both'.\n",
        "    zero_vel_on_target_TF : bool\n",
        "        If True, zero velocity when inside target radius.\n",
        "    \"\"\"\n",
        "    def __init__(self, C=1, alpha=0.8, velocity_rot_method=\"flip\", zero_vel_on_target_TF=False):\n",
        "        super().__init__(C)\n",
        "        self.alpha = alpha\n",
        "        self.velocity_rot_method = velocity_rot_method.lower()\n",
        "        self.zero_vel_on_target_TF = zero_vel_on_target_TF\n",
        "\n",
        "    def _compute_flipped_velocity(self, pos, vel, targ, targsize):\n",
        "        \"\"\"Flip velocity if moving away from the target.\"\"\"\n",
        "        v_flip = vel.copy()\n",
        "        T, D = pos.shape\n",
        "        for t in range(T):\n",
        "            for d in range(D):\n",
        "                if targ[t, d] == -1:\n",
        "                    continue\n",
        "                dist = pos[t, d] - targ[t, d]\n",
        "                hit = abs(dist) < targsize\n",
        "                if self.zero_vel_on_target_TF and hit:\n",
        "                    v_flip[t, d] = 0\n",
        "                elif (dist < 0 and v_flip[t, d] < 0) or (dist > 0 and v_flip[t, d] > 0):\n",
        "                    v_flip[t, d] *= -1\n",
        "        return v_flip\n",
        "\n",
        "    def _compute_rotated_velocity(self, pos, vel, targ, targsize):\n",
        "        \"\"\"Rotate velocity to point toward the target direction.\"\"\"\n",
        "        v_rot = vel.copy()\n",
        "        T, D = pos.shape\n",
        "        targ = targ[:, :D]  # Only keep position dimensions\n",
        "\n",
        "        for t in range(T):\n",
        "            good_dims = targ[t] != -1\n",
        "            if not np.any(good_dims):\n",
        "                continue\n",
        "            vec_to_target = targ[t, good_dims] - pos[t, good_dims]\n",
        "            norm_vec = np.linalg.norm(vec_to_target)\n",
        "            if norm_vec > 1e-5:\n",
        "                speed = np.linalg.norm(v_rot[t, good_dims])\n",
        "                v_rot[t, good_dims] = (vec_to_target / norm_vec) * speed\n",
        "                if self.zero_vel_on_target_TF:\n",
        "                    hit_mask = np.zeros_like(good_dims)\n",
        "                    hit_mask[good_dims] = np.abs(pos[t, good_dims] - targ[t, good_dims]) < targsize\n",
        "                    v_rot[t, hit_mask] = 0\n",
        "        return v_rot\n",
        "\n",
        "    def compute_intended_trajectory(self, y_train, targets, targsize):\n",
        "        \"\"\"\n",
        "        Modifies training trajectories to align velocity with target using selected strategy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_train : ndarray of shape [T, 2*D]\n",
        "            Original training behavior data [position, velocity].\n",
        "        targets : ndarray of shape [T, D]\n",
        "            Target positions.\n",
        "        targsize : scalar\n",
        "            Target size threshold for each time point.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_refit : ndarray of shape [T, 2*D]\n",
        "            Modified training data with position and reoriented velocity.\n",
        "        \"\"\"\n",
        "        T, total_dims = y_train.shape\n",
        "        D = total_dims // 2\n",
        "        pos = y_train[:, :D]\n",
        "        vel = y_train[:, D:]\n",
        "\n",
        "        if self.velocity_rot_method == \"rotate\":\n",
        "            vel_new = self._compute_rotated_velocity(pos, vel, targets, targsize)\n",
        "            pos_new = pos\n",
        "        elif self.velocity_rot_method == \"flip\":\n",
        "            vel_new = self._compute_flipped_velocity(pos, vel, targets, targsize)\n",
        "            pos_new = pos\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid velocity_rot_method. Must be 'flip' or 'rotate'.\")\n",
        "\n",
        "        return np.hstack([pos_new, vel_new])\n",
        "\n",
        "    def fit(self, X_kf_train, y_train, targets=None, targsize=None):\n",
        "        \"\"\"\n",
        "        Fits the Kalman Filter using intention-aligned trajectories if targets and targsize are given.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_kf_train : ndarray\n",
        "            Neural input data.\n",
        "        y_train : ndarray\n",
        "            Original behavioral training data [position, velocity].\n",
        "        targets : ndarray, optional\n",
        "            Target positions corresponding to each training time step.\n",
        "        targsize : ndarray, optional\n",
        "            Target radius threshold for each time step.\n",
        "        \"\"\"\n",
        "        if targets is not None and targsize is not None:\n",
        "            y_train_rotated = self.compute_intended_trajectory(y_train, targets, targsize)\n",
        "        super().fit(X_kf_train, y_train_rotated)\n",
        "\n",
        "    def predict(self, X_kf_test, position_feedback=None):\n",
        "        \"\"\"\n",
        "        Decodes neural data and optionally replaces predicted position with known position feedback.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_kf_test : ndarray\n",
        "            Neural test data.\n",
        "        position_feedback : ndarray, optional\n",
        "            Known position data to replace decoded positions.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        states : ndarray\n",
        "            Decoded behavioral states.\n",
        "        \"\"\"\n",
        "        states = super().predict(X_kf_test)\n",
        "        if position_feedback is not None:\n",
        "            states[:, :position_feedback.shape[1]] = position_feedback\n",
        "        return states\n"
      ],
      "metadata": {
        "id": "LTB9puUO6dGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wiener Filter (Unregularized Linear Regression)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gxVpCVKG2uaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "class WienerFilterDecoder(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Class for the Wiener Filter Decoder\n",
        "\n",
        "    There are no parameters to set.\n",
        "\n",
        "    This simply leverages the scikit-learn linear regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data_preprocessor = StandardScaler() # Z-score neural data\n",
        "        self.targ_transformer  = StandardScaler(with_std=False) # Zero-center target data\n",
        "        self.model = LinearRegression()\n",
        "\n",
        "    def fit(self,X_train,y_train,num_bins_before=3):\n",
        "\n",
        "        \"\"\"\n",
        "        Train Wiener Filter Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train: numpy 2d array of shape [n_samples,n_features]\n",
        "            This is the neural data.\n",
        "            See example file for an example of how to format the neural data correctly\n",
        "\n",
        "        y_train: numpy 2d array of shape [n_samples, n_outputs]\n",
        "            This is the outputs that are being predicted\n",
        "        \"\"\"\n",
        "        # ---------------- Preprocess Training Data --------------\n",
        "        self.num_bins_before = num_bins_before\n",
        "\n",
        "        # Get the covariate matrix that includes spike history from previous bins\n",
        "        X_train, y_train = append_history(X_train, targ_data=y_train,num_bins_before = num_bins_before)\n",
        "\n",
        "        X_train = self.data_preprocessor.fit_transform(X_train)\n",
        "        y_train = self.targ_transformer.fit_transform(y_train)\n",
        "\n",
        "        #----------------------------------------------------------\n",
        "        self.model.fit(X_train, y_train) #Train the model\n",
        "\n",
        "\n",
        "    def predict(self,X_pred):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict outcomes using trained Wiener Cascade Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_pred: numpy 2d array of shape [n_samples,n_features]\n",
        "            This is the neural data being used to predict outputs.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy 2d array of shape [n_samples,n_outputs]\n",
        "            The predicted outputs\n",
        "        \"\"\"\n",
        "        # ----------- Preprocess Test (Prediction) Data -------------\n",
        "        num_bins_before = self.num_bins_before\n",
        "\n",
        "        # Get the covariate matrix that includes spike history from previous bins\n",
        "        X_pred,_ = append_history(X_pred, num_bins_before = num_bins_before)\n",
        "\n",
        "        X_pred = self.data_preprocessor.transform(X_pred) # Z-score neural data\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        y_pred = self.model.predict(X_pred) # Make predictions\n",
        "\n",
        "        y_pred = self.targ_transformer.inverse_transform(y_pred) # Undo zero-centering\n",
        "\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "sC8rG9j42xh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression Decoder\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nloWjoIzGQ9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "class RidgeDecoder(object):\n",
        "    \"\"\"\n",
        "    Ridge Regression-based Decoder.\n",
        "\n",
        "    Includes history stacking and standardization of input features (Z-scoring) and\n",
        "    zero-centering of targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : float\n",
        "            Regularization strength for Ridge regression.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.data_preprocessor = StandardScaler()  # Z-score neural data\n",
        "        self.targ_transformer  = StandardScaler(with_std=False)  # Zero-center target\n",
        "        self.model = Ridge(alpha=self.alpha, max_iter=10000)\n",
        "\n",
        "    def fit(self, X_train, y_train, num_bins_before=3):\n",
        "        \"\"\"\n",
        "        Train the Ridge Wiener Filter decoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray, shape (n_samples, n_features)\n",
        "            Neural activity data.\n",
        "\n",
        "        y_train : ndarray, shape (n_samples, n_outputs)\n",
        "            Target output data (e.g., cursor velocity).\n",
        "\n",
        "        num_bins_before : int\n",
        "            Number of history bins to include before the current bin.\n",
        "        \"\"\"\n",
        "        self.num_bins_before = num_bins_before\n",
        "\n",
        "        # Create history-augmented neural data\n",
        "        X_train_hist, y_train_trimmed = append_history(X_train, y_train, num_bins_before)\n",
        "\n",
        "        # Standardize input and target data\n",
        "        X_train_scaled   = self.data_preprocessor.fit_transform(X_train_hist)\n",
        "        y_train_centered = self.targ_transformer.fit_transform(y_train_trimmed)\n",
        "\n",
        "        # Train Ridge regression model\n",
        "        self.model.fit(X_train_scaled, y_train_centered)\n",
        "\n",
        "    def predict(self, X_pred):\n",
        "        \"\"\"\n",
        "        Predict output using trained decoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_pred : ndarray, shape (n_samples, n_features)\n",
        "            Neural activity data for prediction.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray, shape (n_samples - num_bins_before, n_outputs)\n",
        "            Predicted target outputs.\n",
        "        \"\"\"\n",
        "        num_bins_before = self.num_bins_before\n",
        "\n",
        "        # Apply same history transformation and scaling\n",
        "        X_pred_hist, _ = append_history(X_pred, None, num_bins_before)\n",
        "        X_pred_scaled  = self.data_preprocessor.transform(X_pred_hist)\n",
        "\n",
        "        # Predict and un-center the predictions\n",
        "        y_pred_centered = self.model.predict(X_pred_scaled)\n",
        "        y_pred = self.targ_transformer.inverse_transform(y_pred_centered)\n",
        "\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "7CUfUBfgGT3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Learner (XGBoost)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ahchvJ5gMTM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class XGBoostMultiOutputDecoder(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    XGBoost-based decoder for multidimensional regression using a single model,\n",
        "    with support for input history stacking, optional validation data, and early stopping.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        Maximum depth of trees.\n",
        "    num_round : int\n",
        "        Maximum number of boosting rounds.\n",
        "    eta : float\n",
        "        Learning rate.\n",
        "    num_bins_before : int\n",
        "        Number of past bins to include as time history.\n",
        "    patience : int or None\n",
        "        If set, enables early stopping with validation set (after patience rounds of no performance improvement).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=3, num_round=300, eta=0.3, min_child_weight=2, subsample = 0.5, colsample_bytree=0.8, num_bins_before=3, patience=10,val_verbose=False):\n",
        "        self.max_depth = max_depth\n",
        "        self.num_round = num_round\n",
        "        self.eta = eta\n",
        "        self.min_child_weight = min_child_weight\n",
        "        self.subsample = subsample\n",
        "        self.colsample_bytree = colsample_bytree\n",
        "        self.num_bins_before = num_bins_before\n",
        "        self.patience = patience\n",
        "        self.val_verbose = val_verbose\n",
        "\n",
        "        self.model = None\n",
        "        self.data_preprocessor = StandardScaler()\n",
        "        self.target_centerer = StandardScaler(with_std=False)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Fit model with optional validation for early stopping.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (T, N)\n",
        "            Training neural features.\n",
        "\n",
        "        y : ndarray, shape (T, D)\n",
        "            Training targets.\n",
        "\n",
        "        X_val : ndarray, optional\n",
        "            Validation neural features.\n",
        "\n",
        "        y_val : ndarray, optional\n",
        "            Validation targets.\n",
        "        \"\"\"\n",
        "        X_train_hist, y_train_hist = append_history(X, y, self.num_bins_before)\n",
        "        X_train_scaled = self.data_preprocessor.fit_transform(X_train_hist)\n",
        "        y_train_centered = self.target_centerer.fit_transform(y_train_hist)\n",
        "\n",
        "        dtrain = xgb.DMatrix(X_train_scaled, label=y_train_centered)\n",
        "\n",
        "        evals = []\n",
        "        evals_result = {}\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "            X_val_hist, y_val_hist = append_history(X_val, y_val, self.num_bins_before)\n",
        "            X_val_scaled = self.data_preprocessor.transform(X_val_hist)\n",
        "            y_val_centered = self.target_centerer.transform(y_val_hist)\n",
        "\n",
        "            dval = xgb.DMatrix(X_val_scaled, label=y_val_centered)\n",
        "            evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
        "        else:\n",
        "            evals = [(dtrain, \"train\")]\n",
        "\n",
        "        params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'eval_metric': 'rmse',\n",
        "            'max_depth': self.max_depth,\n",
        "            'eta': self.eta,\n",
        "            'min_child_weight': self.min_child_weight,\n",
        "            'subsample': self.subsample,\n",
        "            'colsample_bytree': self.colsample_bytree,\n",
        "            'verbosity': 0}\n",
        "\n",
        "        self.model = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=self.num_round,\n",
        "            evals=evals,\n",
        "            evals_result=evals_result,\n",
        "            early_stopping_rounds=self.patience if len(evals) > 1 else None,\n",
        "            verbose_eval=self.val_verbose\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict from neural data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (T, N)\n",
        "            Neural input.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray, shape (T - num_bins_before, D)\n",
        "            Predicted targets.\n",
        "        \"\"\"\n",
        "        X_hist, _ = append_history(X, None, self.num_bins_before)\n",
        "        X_scaled = self.data_preprocessor.transform(X_hist)\n",
        "        dtest = xgb.DMatrix(X_scaled)\n",
        "\n",
        "        y_pred_centered = self.model.predict(dtest)\n",
        "        y_pred = self.target_centerer.inverse_transform(y_pred_centered)\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "eLR4JdvdMahs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward Neural Network\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l7UF-x4lPXER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class FeedForwardNNDecoder(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Class for the dense (fully-connected) neural network decoder\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    units: integer or vector of integers, optional, default 400\n",
        "        This is the number of hidden units in each layer\n",
        "        If you want a single layer, input an integer (e.g. units=400 will give you a single hidden layer with 400 units)\n",
        "        If you want multiple layers, input a vector (e.g. units=[400,200]) will give you 2 hidden layers with 400 and 200 units, repsectively.\n",
        "        The vector can either be a list or an array\n",
        "\n",
        "    dropout: decimal, optional, default 0\n",
        "        Proportion of units that get dropped out\n",
        "\n",
        "    num_epochs: integer, optional, default 10\n",
        "        Number of epochs used for training\n",
        "\n",
        "    verbose: binary, optional, default=0\n",
        "        Whether to show progress of the fit after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,units=64, dropout=0.25, batch_size=32, num_epochs=100, verbose=1):\n",
        "         self.dropout    = dropout\n",
        "         self.num_epochs = num_epochs\n",
        "         self.batch_size = batch_size\n",
        "         self.verbose    = verbose\n",
        "\n",
        "         self.num_bins_before = None\n",
        "\n",
        "         #If \"units\" is an integer, put it in the form of a vector\n",
        "         try: #Check if it's a vector\n",
        "             units[0]\n",
        "         except: #If it's not a vector, create a vector of the number of units for each layer\n",
        "             units=[units]\n",
        "         self.units=units\n",
        "\n",
        "         #Determine the number of hidden layers (based on \"units\" that the user entered)\n",
        "         self.num_layers=len(units)\n",
        "\n",
        "         self.data_preprocessor = StandardScaler() # Z-score neural data\n",
        "         self.targ_transformer  = StandardScaler(with_std=False) # Zero-center target data\n",
        "\n",
        "\n",
        "    def fit(self,X_train,y_train, val_data = None, num_bins_before=3, early_stopping_TF = True, patience = 20):\n",
        "\n",
        "        \"\"\"\n",
        "        Train DenseNN Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train: numpy 2d array of shape [n_samples,n_features]\n",
        "            This is the neural data.\n",
        "            See example file for an example of how to format the neural data correctly\n",
        "\n",
        "        y_train: numpy 2d array of shape [n_samples, n_outputs]\n",
        "            This is the outputs that are being predicted\n",
        "        \"\"\"\n",
        "        # ---------------- Preprocess Training Data --------------\n",
        "        self.num_bins_before = num_bins_before\n",
        "        # Get the covariate matrix that includes spike history from previous bins\n",
        "        X_train, y_train = append_history(X_train, targ_data = y_train, num_bins_before = num_bins_before)\n",
        "\n",
        "        X_train = self.data_preprocessor.fit_transform(X_train)\n",
        "        y_train = self.targ_transformer.fit_transform(y_train)\n",
        "\n",
        "        # ----------- Preprocess Validation Data, if Available -----------\n",
        "\n",
        "        callbacks = []\n",
        "        # preprocess validation set\n",
        "        if val_data is not None:\n",
        "          val_data[0],val_data[1] = append_history(val_data[0], targ_data=val_data[1], num_bins_before = num_bins_before)\n",
        "          val_data[0] = val_data[0].reshape(val_data[0].shape[0], -1)\n",
        "          # val_data[0] = val_data[0][num_bins_before:,:]\n",
        "          val_data[0] = self.data_preprocessor.transform(val_data[0])\n",
        "\n",
        "          # val_data[1] = val_data[1][num_bins_before:,:]\n",
        "          val_data[1] = self.targ_transformer.transform(val_data[1])\n",
        "\n",
        "          if early_stopping_TF:\n",
        "            from keras.callbacks import EarlyStopping\n",
        "            early_stop = EarlyStopping(monitor   = 'val_loss',\n",
        "                                        patience = patience,\n",
        "                                        restore_best_weights=True,\n",
        "                                        verbose=0)\n",
        "            callbacks.append(early_stop)\n",
        "\n",
        "        model=Sequential() #Declare model\n",
        "        #Add first hidden layer\n",
        "        model.add(Input(shape=(X_train.shape[1],)))\n",
        "        model.add(Dense(self.units[0])) #Add dense layer\n",
        "        model.add(Activation('relu')) #Add nonlinear (tanh) activation\n",
        "        # if self.dropout!=0:\n",
        "        if self.dropout!=0: model.add(Dropout(self.dropout))  #Dropout some units if proportion of dropout != 0\n",
        "\n",
        "        #Add any additional hidden layers (beyond the 1st)\n",
        "        for layer in range(self.num_layers-1): #Loop through additional layers\n",
        "            model.add(Dense(self.units[layer+1])) #Add dense layer\n",
        "            model.add(Activation('relu')) #Add nonlinear (tanh) activation\n",
        "            if self.dropout!=0: model.add(Dropout(self.dropout)) #Dropout some units if proportion of dropout != 0\n",
        "\n",
        "        #Add dense connections to all outputs\n",
        "        model.add(Dense(y_train.shape[1])) #Add final dense layer (connected to outputs)\n",
        "\n",
        "        #Fit model (and set fitting parameters)\n",
        "        model.compile(loss='mse',optimizer=Adam(learning_rate=1e-4),metrics=['mse']) #Set loss function and optimizer\n",
        "\n",
        "        model.fit(X_train,\n",
        "                  y_train,epochs = self.num_epochs,\n",
        "                  batch_size     = self.batch_size,\n",
        "                  validation_data= val_data,\n",
        "                  callbacks      = callbacks,\n",
        "                  verbose        = self.verbose) #Fit the model\n",
        "\n",
        "        self.model=model\n",
        "\n",
        "    def predict(self,X_pred):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict outcomes using trained DenseNN Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_pred: numpy 2d array of shape [n_samples,n_features]\n",
        "            This is the neural data being used to predict outputs.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy 2d array of shape [n_samples,n_outputs]\n",
        "            The predicted outputs\n",
        "        \"\"\"\n",
        "\n",
        "        # ----------- Preprocess Test (Prediction) Data -------------\n",
        "        num_bins_before = self.num_bins_before\n",
        "\n",
        "        # Get the covariate matrix that includes spike history from previous bins\n",
        "        X_pred, _ = append_history(X_pred, num_bins_before = num_bins_before)\n",
        "\n",
        "        X_pred = self.data_preprocessor.transform(X_pred) #Z-score neural data\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        y_pred = self.model.predict(X_pred) # Make predictions\n",
        "\n",
        "        y_pred = self.targ_transformer.inverse_transform(y_pred) #Undo zero-centering\n",
        "        return y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "pAHG4apDPUhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN-FeedForward Hybrid Neural Network\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EnOGsDmxGlhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, BatchNormalization, Conv1D, Dense,\n",
        "                                     Dropout, Flatten, Permute, ReLU)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "class CNN_FeedForwardNNDecoder:\n",
        "    \"\"\"\n",
        "    CNN + Feedforward Neural Network decoder with built-in history stacking.\n",
        "\n",
        "    This model accepts raw neural data of shape (T, N), appends a time window of\n",
        "    past activity, and uses a combination of 1D convolution and dense layers for regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hidden_size : int\n",
        "        Number of units in each fully connected layer.\n",
        "    ConvSizeOut : int\n",
        "        Number of filters in the Conv1D layer.\n",
        "    num_outputs : int\n",
        "        Number of continuous regression outputs.\n",
        "    learning_rate : float, default=1e-3\n",
        "        Optimizer learning rate.\n",
        "    dropout_rate : float, default=0.5\n",
        "        Dropout rate for regularization.\n",
        "    num_bins_before : int, default=3\n",
        "        Number of history bins to include before the current bin.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size=256, ConvSizeOut=16,\n",
        "                 learning_rate=1e-3, dropout_rate=0, num_bins_before=3):\n",
        "        self.hidden_size     = hidden_size\n",
        "        self.ConvSizeOut     = ConvSizeOut\n",
        "        self.learning_rate   = learning_rate\n",
        "        self.dropout_rate    = dropout_rate\n",
        "        self.num_bins_before = num_bins_before\n",
        "\n",
        "        self.model       = None\n",
        "        self.input_size  = None\n",
        "        self.num_outputs = None\n",
        "\n",
        "        self.used_PCA_TF = False # flag to tell predict() whether or not PCA was used during training\n",
        "\n",
        "        self.data_preprocessor = StandardScaler() # Z-score neural data\n",
        "        self.targ_transformer  = StandardScaler(with_std=False) # Zero-center target data\n",
        "\n",
        "\n",
        "    def _build_model(self, input_size, num_outputs):\n",
        "        \"\"\"\n",
        "        Build the internal Keras model after inferring input/output dimensions.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size : int\n",
        "        num_outputs : int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model : tf.keras.Model\n",
        "        \"\"\"\n",
        "        input_tensor = Input(shape=(self.num_bins_before + 1, input_size), name='Input')\n",
        "        x = BatchNormalization(name='BN0')(input_tensor)\n",
        "        x = Permute((2, 1), name='Permute')(x)  # (batch, channels, time)\n",
        "        x = Conv1D(filters=self.ConvSizeOut, kernel_size=1, name='Conv1')(x)\n",
        "        x = Flatten(name='Flatten')(x)\n",
        "\n",
        "        for i in range(3):\n",
        "            x = Dense(self.hidden_size, kernel_initializer='he_normal', name=f'FC{i+1}')(x)\n",
        "            x = Dropout(self.dropout_rate, name=f'Dropout{i+1}')(x)\n",
        "            x = BatchNormalization(name=f'BN{i+2}')(x)\n",
        "            x = ReLU(name=f'ReLU{i+1}')(x)\n",
        "\n",
        "        output = Dense(num_outputs, kernel_initializer='he_normal', name='Output')(x)\n",
        "        model = Model(inputs=input_tensor, outputs=output, name='CNN_FeedForwardNNDecoder')\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "            loss='mean_squared_error',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "\n",
        "    def fit(self, X_train, y_train, use_PCA_TF=False, PC_thresh=None, batch_size=32, epochs=10,\n",
        "            validation_data = None, verbose  = 1,\n",
        "            early_stopping  = True, patience = 5, checkpoint_path = None):\n",
        "        \"\"\"\n",
        "        Fit the model using raw neural and target data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : ndarray, shape (T, N)\n",
        "            Raw training neural data.\n",
        "        y_train : ndarray, shape (T, D)\n",
        "            Training regression targets.\n",
        "        validation_data : tuple or None\n",
        "            Tuple of (X_val, y_val), both raw.\n",
        "        early_stopping : bool, default=True\n",
        "            Whether to use early stopping.\n",
        "        patience : int, default=5\n",
        "            Number of epochs to wait before stopping.\n",
        "        checkpoint_path : str or None\n",
        "            If given, saves best model weights to this path.\n",
        "        \"\"\"\n",
        "\n",
        "        #------- Run PCA if true ------------\n",
        "        if use_PCA_TF:\n",
        "          self.used_PCA_TF = True\n",
        "          self.num_PCs = None\n",
        "          from sklearn.decomposition import PCA\n",
        "          self.pca_transformer = PCA()\n",
        "          self.pca_transformer.fit(X_train)\n",
        "\n",
        "          if isinstance(PC_thresh,float) and PC_thresh>0:\n",
        "              explained_var_cumsum = np.cumsum(self.pca_transformer.explained_variance_ratio_)\n",
        "              self.num_PCs = np.where(explained_var_cumsum <= PC_thresh)[0][-1]\n",
        "          elif isinstance(PC_thresh,int) and PC_thresh>0:\n",
        "              self.num_PCs = PC_thresh\n",
        "\n",
        "          X_train = self.pca_transformer.transform(X_train)[:, :self.num_PCs]\n",
        "\n",
        "        # ---------- scale neural features ---------------\n",
        "        X_train   = self.data_preprocessor.fit_transform(X_train)\n",
        "        y_train   = self.targ_transformer.fit_transform(y_train)\n",
        "\n",
        "        # ------------- Append history and infer shapes --------------\n",
        "        X_hist, y_hist = append_history_cnn(X_train, y_train, self.num_bins_before)\n",
        "\n",
        "        # if self.model is None:\n",
        "        #     self.input_size  = X_hist.shape[2]\n",
        "        #     self.num_outputs = y_hist.shape[1]\n",
        "        #     self.model = self._build_model(self.input_size, self.num_outputs)\n",
        "        self.input_size  = X_hist.shape[2]\n",
        "        self.num_outputs = y_hist.shape[1]\n",
        "        self.model = self._build_model(self.input_size, self.num_outputs)\n",
        "\n",
        "        # Handle validation data\n",
        "        val_data = None\n",
        "        if validation_data is not None:\n",
        "            X_val, y_val = validation_data\n",
        "\n",
        "            if self.used_PCA_TF:\n",
        "              X_val = self.pca_transformer.transform(X_val)[:, :self.num_PCs]\n",
        "\n",
        "            X_val = self.data_preprocessor.transform(X_val)\n",
        "            y_val = self.targ_transformer.transform(y_val)\n",
        "\n",
        "            X_val_hist, y_val_hist = append_history_cnn(X_val, y_val, self.num_bins_before)\n",
        "            val_data = (X_val_hist, y_val_hist)\n",
        "\n",
        "        # Setup callbacks\n",
        "        callbacks = []\n",
        "        if early_stopping and val_data:\n",
        "            callbacks.append(EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True))\n",
        "        if checkpoint_path:\n",
        "            callbacks.append(ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss'))\n",
        "\n",
        "        return self.model.fit(\n",
        "            X_hist, y_hist,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=val_data,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "    def predict(self, X_pred):\n",
        "        \"\"\"\n",
        "        Predict using the trained model from raw neural input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_pred : ndarray, shape (T, N)\n",
        "            Raw neural data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : ndarray, shape (T - num_bins_before, D)\n",
        "            Predicted target values.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.used_PCA_TF:\n",
        "          X_pred = self.pca_transformer.transform(X_pred)[:, :self.num_PCs]\n",
        "\n",
        "        X_pred = self.data_preprocessor.transform(X_pred)\n",
        "\n",
        "        X_hist, _ = append_history_cnn(X_pred, None, self.num_bins_before)\n",
        "        y_pred = self.model.predict(X_hist)\n",
        "        y_pred = self.targ_transformer.inverse_transform(y_pred)\n",
        "        return y_pred\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Print the model architecture summary.\"\"\"\n",
        "        return self.model.summary()\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"\n",
        "        Save the full model to a file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            Path to save the model (e.g., 'model.h5')\n",
        "        \"\"\"\n",
        "        self.model.save(path)\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        \"\"\"\n",
        "        Load weights from a checkpoint file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            Path to weights file (e.g., 'best_weights.h5')\n",
        "        \"\"\"\n",
        "        self.model.load_weights(path)\n"
      ],
      "metadata": {
        "id": "TSFizvZWGug7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Decoder\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NRh9AJr4iSKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Input\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "class LSTMRegression(object):\n",
        "    \"\"\"\n",
        "    LSTM regression decoder with internal support for time-lagged history.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units : int\n",
        "        Number of LSTM units\n",
        "\n",
        "    dropout : float\n",
        "        Dropout probability\n",
        "\n",
        "    batch_size : int\n",
        "        Mini-batch size for training\n",
        "\n",
        "    num_epochs : int\n",
        "        Number of training epochs\n",
        "\n",
        "    verbose : int\n",
        "        Verbosity level (0 = silent, 1 = progress bar)\n",
        "    \"\"\"\n",
        "    def __init__(self, units=400, dropout=0, batch_size=32, num_epochs=100, verbose=1):\n",
        "        self.units       = units\n",
        "        self.dropout     = dropout\n",
        "        self.batch_size  = batch_size\n",
        "        self.num_epochs  = num_epochs\n",
        "        self.verbose     = verbose\n",
        "\n",
        "        self.data_preprocessor = StandardScaler()\n",
        "        self.targ_transformer  = StandardScaler(with_std=False)\n",
        "        self.num_bins_before   = None\n",
        "\n",
        "    def fit(self, X, y, val_data=None, num_bins_before=10, early_stopping_TF=True, patience=20):\n",
        "        \"\"\"\n",
        "        Train the LSTM model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray (T, N)\n",
        "            2D neural data\n",
        "\n",
        "        y : np.ndarray (T, m)\n",
        "            2D target data\n",
        "\n",
        "        val_data : tuple (X_val, y_val)\n",
        "            Validation set (optional)\n",
        "\n",
        "        num_bins_before : int\n",
        "            Number of time bins before the current one to include in history\n",
        "\n",
        "        early_stopping_TF : bool\n",
        "            Enable early stopping\n",
        "\n",
        "        patience : int\n",
        "            Patience for early stopping\n",
        "        \"\"\"\n",
        "        self.num_bins_before = num_bins_before\n",
        "\n",
        "        # ---- Build history-expanded inputs ----\n",
        "        X, y = append_history(X, y, num_bins_before=num_bins_before, mode='expand')\n",
        "        X = self._reshape_and_scale_input(X, fit=True)\n",
        "        y = self.targ_transformer.fit_transform(y)\n",
        "\n",
        "        # ---- Validation set ----\n",
        "        callbacks = []\n",
        "        if val_data is not None:\n",
        "            X_val, y_val = val_data\n",
        "            X_val, y_val = append_history(X_val, y_val, num_bins_before=num_bins_before, mode='expand')\n",
        "            X_val = self._reshape_and_scale_input(X_val, fit=False)\n",
        "            y_val = self.targ_transformer.transform(y_val)\n",
        "\n",
        "            if early_stopping_TF:\n",
        "                callbacks.append(EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True))\n",
        "\n",
        "            val_data = (X_val, y_val)\n",
        "\n",
        "        # ---- Define model ----\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(X.shape[1], X.shape[2])))\n",
        "        model.add(LSTM(self.units, dropout=self.dropout, recurrent_dropout=self.dropout))\n",
        "        if self.dropout > 0:\n",
        "            model.add(Dropout(self.dropout))\n",
        "        model.add(Dense(y.shape[1]))\n",
        "\n",
        "        model.compile(optimizer=RMSprop(), loss='mse', metrics=['mse'])\n",
        "\n",
        "        model.fit(X, y,\n",
        "                  validation_data=val_data,\n",
        "                  epochs=self.num_epochs,\n",
        "                  batch_size=self.batch_size,\n",
        "                  callbacks=callbacks,\n",
        "                  verbose=self.verbose)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict from trained model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : np.ndarray (T, N)\n",
        "            2D neural data (raw, unwindowed)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : np.ndarray (T_valid, m)\n",
        "            Predicted outputs\n",
        "        \"\"\"\n",
        "        X, _ = append_history(X, None, num_bins_before=self.num_bins_before, mode='expand')\n",
        "        X = self._reshape_and_scale_input(X, fit=False)\n",
        "        y_pred = self.model.predict(X)\n",
        "        return self.targ_transformer.inverse_transform(y_pred)\n",
        "\n",
        "    def _reshape_and_scale_input(self, X, fit=False):\n",
        "        \"\"\"\n",
        "        Flatten and scale each trial's neural window as a row,\n",
        "        then reshape it back to 3D for LSTM input.\n",
        "        \"\"\"\n",
        "        n_samples, n_time, n_feat = X.shape\n",
        "        X_reshaped = X.reshape(n_samples, -1)\n",
        "\n",
        "        if fit:\n",
        "            X_scaled = self.data_preprocessor.fit_transform(X_reshaped)\n",
        "        else:\n",
        "            X_scaled = self.data_preprocessor.transform(X_reshaped)\n",
        "\n",
        "        return X_scaled.reshape(n_samples, n_time, n_feat)\n"
      ],
      "metadata": {
        "id": "RHSVWES3iXi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}